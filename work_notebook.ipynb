{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a768d9",
   "metadata": {},
   "source": [
    "<p>Содержание:</p>\n",
    "<p>1  Описание работы</p>\n",
    "<p>2  Загрузка необходимых библиотек</p>\n",
    "<p>3  Загрузка, анализ и предобработка данных</p>\n",
    "<p>3.1  Загрузка excel данных</p>\n",
    "<p>3.2  Обработка целевых меток</p>\n",
    "<p>3.3  Обработка дубликатов</p>\n",
    "<p>3.4  Загрузка субтитров и словаря</p>\n",
    "<p>3.5  Обработка пропусков</p>\n",
    "<p>3.6  Оценка дисбаланса классов</p>\n",
    "<p>4  Препроцессинг данных</p>\n",
    "<p>4.1  Разбивка данных на обучающую и тестовую выборки</p>\n",
    "<p>4.2  Векторизация</p>\n",
    "<p>5  Обучение модели MultinomialNB</p>\n",
    "<p>5.1  Выбор метрики качества</p>\n",
    "<p>5.2  Построение pipiline и обучение</p>\n",
    "<p>5.3  Оценка модели на тестовой (отложенной) выборке</p>\n",
    "<p>5.4  Выводы по результатам работы модели</p>\n",
    "<p>5.5  Обучение модели MultinomialNB на всех данных. Сохранение модели.</p>\n",
    "<p>6  Выводы и рекомендации</p>\n",
    "<p>Описание работыv\n",
    "<p>Проект:</p>\n",
    "<p>Запрос сформирован тем, что просмотр фильмов на оригинальном языке - это популярный и действенный метод упражнений по изучению иностранных языков. Важно выбрать фильм, который подходит студенту по уровню сложности, т.е. студент понимал 50-70 % диалогов. Чтобы выполнить это условие, преподаватель должен посмотреть фильм и решить, какому уровню он соответствует. Однако это требует больших временных затрат.</p>\n",
    "\n",
    "<p>Заказчику необходимо:\n",
    "\n",
    "<p>разработать ML решение для автоматического определения уровня сложности англоязычных фильмов;\n",
    "развернуть приложение для демонтрации работы ML решения (заказчик допустил возможность использваоние пакета Streamlit).</p>\n",
    "\n",
    "<p>Исходные данные:\n",
    "\n",
    "<p>размеченный датасет с названиями фильмов в формате excel, субтитрами и меткой уровня сложности языка (A1/A2/B1/B2/C1/C2). Предварительно данные в таблице уже обработаны (добавлены недостающие фильмы);</p>\n",
    "<p>файлы субтитров в формате .srt, отсортированные по каталогам в соответствии с уровнем сложности. Предварительно все субтитры были перенесены в общую папку для 'D:/English_films_level/Datasets/Subtitles загрузки';</p>\n",
    "<p>словари Oxford (на 3000 и 5000 тыс.слов), в которых слова на английском сгруппированны по уровню сложности.</p>\n",
    "<p>План работы:</p>\n",
    "\n",
    "<p>загрузка необходимых библиотек;</p>\n",
    "<p>загрузка и ознакомление с данными;</p>\n",
    "<p>предобработка данных (очистка от дубликатов, проверка наличия разметки для обучающих данных, определение исходного количества представленных данных, очитска текста субтитров, разбивка на обучающую и тестовую выборки);</p>\n",
    "<p>препроцессинг данных (преобразование данных для обучения модели);</p>\n",
    "<p>определение метрики качества;</p>\n",
    "<p>обучение модели с побором гиперпараметров;</p>\n",
    "<p>оценка модели на тестовой выборке;</p>\n",
    "<p>развертывание модели для демонтрации работы с использованием библиотеки Streamlit;</p>\n",
    "<p>рекомендации к улучшению проекта.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90652b",
   "metadata": {},
   "source": [
    "## Загрузка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "1a124c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pysrt in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: chardet in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from pysrt) (4.0.0)\n",
      "Requirement already satisfied: spacy in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (22.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: en_core_web_sm in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from en_core_web_sm) (3.5.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.4.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (8.1.10)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (65.6.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.7.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (5.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.10.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.23.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.1.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (4.64.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (22.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.0.12)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en_core_web_sm) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.1.1)\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (22.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# установим недостающие пакеты\n",
    "!pip install pysrt\n",
    "!pip install spacy\n",
    "!pip install en_core_web_sm\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# импортируем общие библиотеки\n",
    "import os\n",
    "import numpy as np               \n",
    "import pandas as pd              \n",
    "import pysrt                     \n",
    "import spacy                     \n",
    "import re\n",
    "import warnings \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# импортируем библиотеки sklearn \n",
    "from sklearn.datasets import load_files  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3cad0",
   "metadata": {},
   "source": [
    "Загрузка excel данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "id": "580df571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сразу зададим константу случайного состояния для повторяемости результатов  \n",
    "RAND_ST = 777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "cff6e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df\n",
    "# загрузим данные из excel таблицы\n",
    "df = pd.read_excel('D:/English_films_level/Datasets/movies_labels.xlsx', index_col='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "8c3e681d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All_dogs_go_to_heaven(1989)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An_American_tail(1986)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Babe(1995)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Back_to_the_future(1985)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Banking_On_Bitcoin(2016)</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Movie   Level\n",
       "id                                          \n",
       "0          10_Cloverfield_lane(2016)      B1\n",
       "1   10_things_I_hate_about_you(1999)      B1\n",
       "2               A_knights_tale(2001)      B2\n",
       "3               A_star_is_born(2018)      B2\n",
       "4                      Aladdin(1992)  A2/A2+\n",
       "5        All_dogs_go_to_heaven(1989)  A2/A2+\n",
       "6             An_American_tail(1986)  A2/A2+\n",
       "7                         Babe(1995)  A2/A2+\n",
       "8           Back_to_the_future(1985)  A2/A2+\n",
       "9           Banking_On_Bitcoin(2016)      C1"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "59f8a435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 241 entries, 0 to 240\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Movie   241 non-null    object\n",
      " 1   Level   241 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e6100",
   "metadata": {},
   "source": [
    "Таблица excel содержит 241 размеченных фильмов/сериалов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934c3ad0",
   "metadata": {},
   "source": [
    "## Обработка целевых меток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "bba97fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B1', 'B2', 'A2/A2+', 'C1', 'B1, B2', 'A2/A2+, B1', 'A2'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# определим какие целевые метки содержат данные \n",
    "df['Level'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2959d3f3",
   "metadata": {},
   "source": [
    "В данных присутствуют множественные метки для одного и того же фильма (например 'A2/A2+'). Зададим для таких фильмов единственную метку, соответсвующую максимальному значению уровня сложности.\n",
    "\n",
    "Закодируем метки числовыми пордяками от 1 до 4.\n",
    "\n",
    "Так же отмечено отсутствие фильмов с меткой A1, соответствующих самому легкому уровню сложности. После знакомства с представленными фильмами можно заметить, что таким фильмам (точнее мультикам) присвоена метка A2 (например для мультика 'Toy_story(1995)' присвоена метка A2, несмотря на то что многими специалистами уровень сложности определен как A1). В итоге предлагается придерживаться исходных размеченных данных, представленных Заказчиком и исключить из предсказания метку A1 и считать что она объединена с меткой A2.\n",
    "\n",
    "Важное отступление: нельзя исключать субъективность разметки данных в фильмах. Каждый специалист может немного по-разному определять уровень сложности понимания английского. В таком случае можно ожидать смещения в предсказании любой модели, так как субъективность часто вносит неустранимую ошибку в предиктивность модели. С большой долей вероятности модель будет допускать ошибки в предсказании. Особенно на граничных значениях меток (например ошибаться в присвоении метки A2 или B1, B1 или B2). Такие ошибки впринципе допустимы в отношении поставленной задачи, так как позволяют добиться того, чтобы студент понимал 50-70 % диалогов даже при ошибочном предсказании модели в сторону присвоения более низкой метки, так как различия между такими классами менее существенны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "d77031e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# определим словарь меток с кодированием значений\n",
    "label_dict = {'A2': 1,\n",
    "              'A2/A2+': 1,\n",
    "              'B1': 2,\n",
    "              'A2/A2+, B1': 2,\n",
    "              'B2': 3,\n",
    "              'B1, B2': 3,\n",
    "              'C1': 4}\n",
    "# заменим метки числовыми значениями\n",
    "df = df.replace(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252ea28",
   "metadata": {},
   "source": [
    "## Обработка дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "d5e3f53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Powder(1995)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Inside_out(2015)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Inside_out(2015)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Powder(1995)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>The_terminal(2004)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The_terminal(2004)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Movie  Level\n",
       "id                           \n",
       "38        Powder(1995)      2\n",
       "43    Inside_out(2015)      2\n",
       "44    Inside_out(2015)      2\n",
       "68        Powder(1995)      2\n",
       "83  The_terminal(2004)      2\n",
       "99  The_terminal(2004)      2"
      ]
     },
     "execution_count": 863,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверим данные на наличие дубликатов\n",
    "df[df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b760554",
   "metadata": {},
   "source": [
    "В данных присутствует 3 дубликата. Удалим их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "id": "1f82b726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238, 2)"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# удалим дубликаты \n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4847e3f",
   "metadata": {},
   "source": [
    "## Загрузка субтитров и словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa94b1",
   "metadata": {},
   "source": [
    "Далее загрузим субтитры к фильмам из папки '/Datasets/Subtitles'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "id": "b13ef1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество названий фильмов в папке с субтитрами: 115\n"
     ]
    }
   ],
   "source": [
    "# загрузим в список имена файлов из папки с субтитрами\n",
    "films_name = os.listdir(path= 'D:/English_films_level/Datasets/Subtitles/')\n",
    "# определим количество названий фильмов\n",
    "print(f'Количество названий фильмов в папке с субтитрами: {len(films_name)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "id": "cf7eb63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество фильмов, имеющих метку и субтитры: 106\n"
     ]
    }
   ],
   "source": [
    "# проверим для скольких фильмов, имеющих метку из таблицы, предоставлены субтитры \n",
    "films_filtr = set(films_name) & set(df['Movie'] + '.srt')\n",
    "print(f'Количество фильмов, имеющих метку и субтитры: {len(films_filtr)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f78b73",
   "metadata": {},
   "source": [
    "Далее загрузка субтитров в датафрейм будет осуществлена с применением функции очистки текста. Так же будет загружен словарь Classic Oxford для дальнейшего подсчета количества уникальных лемм, содержащихся в каждом тексте субтитров согласно уровню сложности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "id": "7504f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del american_oxford\n",
    "oxford = load_files('D:/English_films_level/Datasets/Oxford/Classic Oxford/', shuffle=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "2302fbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a, an \\r\\nabout \\r\\nabove \\r\\nacross'"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oxford.data[0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "7301bbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "      <th>subs</th>\n",
       "      <th>A1_lemma_cnt</th>\n",
       "      <th>A2_lemma_cnt</th>\n",
       "      <th>B1_lemma_cnt</th>\n",
       "      <th>B2_lemma_cnt</th>\n",
       "      <th>C1_lemma_cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All_dogs_go_to_heaven(1989)</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An_American_tail(1986)</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Babe(1995)</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Back_to_the_future(1985)</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Banking_On_Bitcoin(2016)</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Movie  Level subs  A1_lemma_cnt  A2_lemma_cnt  \\\n",
       "id                                                                             \n",
       "0          10_Cloverfield_lane(2016)      2                0.0           0.0   \n",
       "1   10_things_I_hate_about_you(1999)      2                0.0           0.0   \n",
       "2               A_knights_tale(2001)      3                0.0           0.0   \n",
       "3               A_star_is_born(2018)      3                0.0           0.0   \n",
       "4                      Aladdin(1992)      1                0.0           0.0   \n",
       "5        All_dogs_go_to_heaven(1989)      1                0.0           0.0   \n",
       "6             An_American_tail(1986)      1                0.0           0.0   \n",
       "7                         Babe(1995)      1                0.0           0.0   \n",
       "8           Back_to_the_future(1985)      1                0.0           0.0   \n",
       "9           Banking_On_Bitcoin(2016)      4                0.0           0.0   \n",
       "\n",
       "    B1_lemma_cnt  B2_lemma_cnt  C1_lemma_cnt  \n",
       "id                                            \n",
       "0            0.0           0.0           0.0  \n",
       "1            0.0           0.0           0.0  \n",
       "2            0.0           0.0           0.0  \n",
       "3            0.0           0.0           0.0  \n",
       "4            0.0           0.0           0.0  \n",
       "5            0.0           0.0           0.0  \n",
       "6            0.0           0.0           0.0  \n",
       "7            0.0           0.0           0.0  \n",
       "8            0.0           0.0           0.0  \n",
       "9            0.0           0.0           0.0  "
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# зададим регулярнеы выражения для очистки текста\n",
    "HTML = re.compile(r'<.*?>') # html тэги меняем на пробел\n",
    "TAG = r'{.*?}' # тэги меняем на пробел\n",
    "COMMENTS = r'[\\(\\[][A-Za-z ]+[\\)\\]]' # комменты в скобках меняем на пробел\n",
    "LETTERS = r'[^a-zA-Z\\.,!? ]' # все что не буквы меняем на пробел \n",
    "SPACES = r'([ ])\\1+' # повторяющиеся пробелы меняем на один пробел\n",
    "DOTS = r'[\\.]+' # многоточие меняем на точку\n",
    "# SYMB = r\"[^\\w\\d'\\s]\" # знаки препинания кроме апострофа\n",
    "\n",
    "# напишем функцию для очистки субтитров\n",
    "def clean_subs(subs):\n",
    "    subs = subs[1:] # удаляем первый рекламный субтитр\n",
    "    txt = re.sub(HTML, ' ', subs.text) # html тэги меняем на пробел\n",
    "    txt = re.sub(COMMENTS, ' ', txt) # комменты в скобках меняем на пробел\n",
    "    txt = re.sub(LETTERS, ' ', txt) # все что не буквы меняем на пробел\n",
    "    txt = re.sub(DOTS, r'.', txt) # многоточие меняем на точку\n",
    "    txt = re.sub(SPACES, r'\\1', txt) # повторяющиеся пробелы меняем на один пробел\n",
    "    # txt = re.sub(SYMB, '', txt) # знаки препинания кроме апострофа на пустую строку\n",
    "    txt = re.sub('www', '', txt) # кое-где остаётся www, то же меняем на пустую строку\n",
    "    txt = txt.lstrip() # обрезка пробелов слева\n",
    "    txt = txt.encode('ascii', 'ignore').decode() # удаляем все что не ascii символы   \n",
    "    txt = txt.lower() # текст в нижний регистр\n",
    "    return txt\n",
    "\n",
    "# функция возвращающая количество уникальных лемм\n",
    "# в тексте субтитров по каждому уровню\n",
    "def lemma_count(lemmas, oxf, cat):\n",
    "    func_dict = {'A1': 0,\n",
    "                 'A2': 1,\n",
    "                 'B1': 2,\n",
    "                 'B2': 3,\n",
    "                 'C1': 4}\n",
    "    level = func_dict[cat]\n",
    "    oxf_word_list = oxf[level].split()\n",
    "    words = [lemma for lemma in lemmas if lemma in oxf_word_list]\n",
    "\n",
    "    return len(set(words))\n",
    "\n",
    "# загрузим для каждого фильма субтитры с использованием библиотеки pysrt\n",
    "for film in films_filtr:\n",
    "    try: \n",
    "        subs = pysrt.open(f'D:/English_films_level/Datasets/Subtitles/{film}')\n",
    "        \n",
    "    except:\n",
    "        subs = pysrt.open(f'D:/English_films_level/Datasets/Subtitles/{film}', encoding='iso-8859-1')\n",
    " \n",
    "    # вызов функии для очистки текста\n",
    "    cln_subs = clean_subs(subs)\n",
    "    df.loc[df['Movie'] == film[:-4], 'subs'] = cln_subs\n",
    "    \n",
    "    # используем библиотеку spacy для лемматизации \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(cln_subs)\n",
    "    lemma_list = [token.lemma_ for token in doc]\n",
    "    \n",
    "    # в цикле по каждой метке запишем в датафрейм кол-во уникальных лемм\n",
    "    for lvl in ['A1', 'A2', 'B1', 'B2', 'C1']:\n",
    "        df.loc[df['Movie'] == film[:-4], lvl+'_lemma_cnt'] = lemma_count(lemma_list, oxford.data, lvl)\n",
    "                   \n",
    "    \n",
    "# выведем первые 10 записей\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57581ada",
   "metadata": {},
   "source": [
    "## Обработка пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a0f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# найдем пропуски в данных после загрузки\n",
    "df[df['subs'].isna()].sort_values('Movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d53a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выдедем отсоритрованный список фильмов  для которых отсутствую субтитры в папке `'/Datasets/Subtitles'`\n",
    "sorted(set(df['Movie'] + '.srt') - set(films_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabda07",
   "metadata": {},
   "source": [
    "Фильмы, для которых в датафрейме отсутствуют субтитры, это те же фильмы, для которых действительно в папке отсутствуют файлы .srt (ошибка в загрузке данных исключается). Удалим их из датафрейма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8422659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# удалим фильмы с пропущенными субтитрами\n",
    "df.dropna(inplace=True)\n",
    "# проверим удаление\n",
    "df[df['subs'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40dde7",
   "metadata": {},
   "source": [
    "## Оценка дисбаланса классов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776fb4cb",
   "metadata": {},
   "source": [
    "Для классификации и определения метрики качества важно проверять данные на наличие дисбаланса целевых классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "788dee8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    39\n",
       "3    37\n",
       "1    25\n",
       "4     6\n",
       "Name: Level, dtype: int64"
      ]
     },
     "execution_count": 822,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# определим количество фильмов, представленных в кажой категории сложности\n",
    "df['Level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80430e56",
   "metadata": {},
   "source": [
    "В данных присутствует сильный дисбаланс по целевым классам (меткам):\n",
    "\n",
    "129 фильмов с уровнем сложности 3 (метка B2);\n",
    "56 фильмов с уровнем сложности 2 (метка B1);\n",
    "39 фильмов с уровнем сложности 4 (метка C1);\n",
    "32 фильма с уровнем сложности 1 (метка A2).\n",
    "Вывод: для борьбы с дисбалансом классов есть несколько методов, которые не являются универсальными по своей сути и каждый должен определяться исходя из требований задачи и исходных данных. Так undersampling (выравнивание выборки за счет снижения количества фильмов с мажоритарными классами) недопустим в текущих условиях и так малого количества исходных данных. В данном случае снижается общая вариативность данных, которая необходима модели машинного обучения для нахождения признаков и их связей с правильным предсказанием целевой метрики. В рамках решения задач NLP малое количество обучающих данных приводит к более 'скудному' словарю. Отсутствие 'богатства' словаря часто приводит к неверному предсказанию модели на новых текстах, которые состоят из слов не вошедших в 'vocabulary' обученной модели.\n",
    "\n",
    "Таким образом в данном случае было бы уместнее применить upsampling (выравнивание выборки за счет увеличения методом дублирования фильмов с миноритарными классами). Но с другой стороны увеличение выборки за счет дублирования текста в миноритарных классах не приведет к увеличению словаря модели и как следствие не увеличит качество прогнозов. В таком случае предлагается оставить выборку без изменений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90762d",
   "metadata": {},
   "source": [
    "## Препроцессинг данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26423633",
   "metadata": {},
   "source": [
    "## Разбивка данных на обучающую и тестовую выборки\n",
    "\n",
    "Разбивка данных будет произведена в соотношении обучающая/тестовая выборка - 80/20 %. Важным моментом который стоит учитывать - сохранение соотношения баланса классов в обеих выборках. Для этого будет использован параметр 'stratify'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "id": "fc71ceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: ((85, 6), (85,))\n",
      "Размер тестовой выборки: ((22, 6), (22,))\n"
     ]
    }
   ],
   "source": [
    "# разобъем данные на валидационную и тестовую выборки\n",
    "df_train, df_test = train_test_split(df, random_state=RAND_ST, test_size=.2, stratify=df['Level'])\n",
    "# определим независимые признаки и зависимую целевую метку\n",
    "X_train = df_train.drop(['Level', 'Movie'], axis=1)\n",
    "y_train = df_train['Level']\n",
    "X_test = df_test.drop(['Level', 'Movie'], axis=1)\n",
    "y_test = df_test['Level']\n",
    "# определим размеры обучающей и тестовой выборки\n",
    "print(f'Размер обучающей выборки: {X_train.shape, y_train.shape}')\n",
    "print(f'Размер тестовой выборки: {X_test.shape, y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe50f07",
   "metadata": {},
   "source": [
    "## Векторизация\n",
    "\n",
    "Векторизуем текст субтитров с помощью 'CountVectorizer' с гиперпараметром min_df=4, чтобы исключить специфические слова для определенных фильмов, которые не несут явной языковой нагрузки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "f2eac76b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[826], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# векторизуем текст субтитров обучающей и тестовой выборки\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m vect \u001b[38;5;241m=\u001b[39m \u001b[43mCountVectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m X_train_vect \u001b[38;5;241m=\u001b[39m vect\u001b[38;5;241m.\u001b[39mtransform(X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m X_test_vect \u001b[38;5;241m=\u001b[39m vect\u001b[38;5;241m.\u001b[39mtransform(X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;124;03m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m \n\u001b[0;32m   1325\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m        Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1293\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m         )\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# векторизуем текст субтитров обучающей и тестовой выборки\n",
    "vect = CountVectorizer(min_df=4).fit(X_train['subs'])\n",
    "X_train_vect = vect.transform(X_train['subs'])\n",
    "X_test_vect = vect.transform(X_test['subs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d996356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выведем результаты (признаки) векторизации данных\n",
    "feature_names = vect.get_feature_names_out()\n",
    "print('Количество признаков: {}'.format(len(feature_names)))\n",
    "print('Первые 20 признаков:\\n{}'.format(feature_names[:20]))\n",
    "print('Признаки с 2000 по 2060:\\n{}'.format(feature_names[2000:2060]))\n",
    "print('Каждый 100-й признак:\\n{}'.format(feature_names[::100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6953a3",
   "metadata": {},
   "source": [
    "## Обучение модели MultinomialNB\n",
    "\n",
    "Для обучения модели на текстовых данных будет использована модель наивного байесовского классификатора MultinomialNB. Такая модель хорошо подходит для высокоразмерных данных, обучается довольно быстро при назначительно меньшем качестве в отличии от линейной модели.\n",
    "\n",
    "Обучение модели будет проводится с использованием GridSearcCV для подбора гиперпараметра 'alpha' с кроссвалидацией на 5-и фолдах. Во избежание утечки данных будет использован Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9a3b11",
   "metadata": {},
   "source": [
    "## Выбор метрики качества\n",
    "\n",
    "В связи с тем, что:\n",
    "\n",
    "в данных обнаружен дисбаланс классов;\n",
    "в текущей задаче является важным как точность ('precision') так и полнота ('recall') предсказания модели;\n",
    "лучшей метрикой для подгонки и оценки модели принимается средне-гармоническое точности и полноты - F1-мера. Для мультиклассификации будет использована взвешенная F1-мера (F1_weighted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию для построения матрицы ошибок\n",
    "def plot_confusion_matrix(y_test, y_preds, model):\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    cm = confusion_matrix(y_test, y_preds)\n",
    "    cmp = ConfusionMatrixDisplay(cm, display_labels = model.classes_ )\n",
    "    cmp.plot(ax=ax)\n",
    "    plt.suptitle('Матрица ошибок', y=0.92)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa870c",
   "metadata": {},
   "source": [
    "## Построение pipiline и обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699ee560",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# выделим отдельную категорию признаков\n",
    "other_colls = X_train.columns[1:]\n",
    "\n",
    "# инициализируем вектор-объект\n",
    "vector = CountVectorizer(min_df=4)\n",
    "\n",
    "# создадим препроцессор для групп признаков\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('other', 'passthrough', other_colls),\n",
    "    ('txt', vector, 'subs')\n",
    "])\n",
    "\n",
    "# инициализируем модель MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# создадим pipeline\n",
    "pipe = Pipeline(steps=[('prep', preprocessor),\n",
    "                       ('clf', classifier)])\n",
    "\n",
    "# обучим модель с подбором гиперпараметра alpha на 5 фолдах\n",
    "param_grid = {'clf__alpha': np.arange(0.001, 0.3, 0.002)}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='f1_weighted')\n",
    "grid.fit(X_train, y_train)\n",
    "# выведем лучший score и значение гиперпараметра\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b9ba76",
   "metadata": {},
   "source": [
    "Лучшая модель показала результат - 0.658 (F1_weighted) при подобранном гиперпараметре 'alpha' равном 0.137.\n",
    "\n",
    "Оценим модель на тестовой выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1012098",
   "metadata": {},
   "source": [
    "## Оценка модели на тестовой (отложенной) выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d378474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получим предсказания на тестовой выборке\n",
    "predictions = grid.predict(X_test)\n",
    "# выведем раезльтаты оценки по метрике F1-weighted\n",
    "print(f'Метрика F1-weighted на обучающей выборке:{f1_score(grid.predict(X_train), y_train, average=\"weighted\")}')\n",
    "print(f'Метрика F1-weighted на тестовой выборке:{f1_score(predictions, y_test, average=\"weighted\")}')\n",
    "print('-'*50)\n",
    "# выведем классификационную таблицу и матрицу ошибок\n",
    "print(classification_report(y_test, predictions))\n",
    "plot_confusion_matrix(y_test, predictions, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c85fb4a",
   "metadata": {},
   "source": [
    "Лучшая модель показала результат - 0.63 (F1_weighted) на тестовой выборке при подобранном гиперпараметре 'alpha' равном 0.137."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24179bd3",
   "metadata": {},
   "source": [
    "## Выводы по результатам работы модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447bcd63",
   "metadata": {},
   "source": [
    "Из оценки классификатора на тестовой выборке можно сделать следующие выводы:\n",
    "\n",
    "как и ожидалось, модель чаще всего ошибается на граничных метках. Например модель часто назначает фильмам с меткой B2 метку B1. По условиям задачи где, студенты должны минимум понимать 50-70% диалогов, ошибка прогнозирования в сторону наименьшего класса допустима в пределах граничных классов;\n",
    "хуже всего классифицированы фильмы с метками A1 и B1. В первом случае за счет низкого значения метрики recall (среди небольшого количества фильмов представленных в этом классе меток (7 фильмов), всего 2 фильма размечены верно). Во втором случае за счет низкого значения метрики precision (модель часто ошибается в присвоении метки B1, т.е. среди всех размеченных фильмов с присвоением метки B1 (21 фильм) только 8 размечены верно.\n",
    "Использование дополнительно созданных признаков, с количеством уникальных слов из классического словаря Oxford в каждом фильме, не дали значительного прироста в качестве предсказаний, что может подтверждать предположение о субъективности разметки представленных Заказчиком фильмов, в силу предвзятости специалистов по английскому языку в отношении сложности диалогов.\n",
    "\n",
    "В целом с учетом предположения о субъективности разметки представленных Заказчиком фильмов, а так же использования быстрого и простого наивного Байесовского классификатора, удалось достичь приемлемых результатов. Взвешенная метрика F1-weighted на тестовой выборке составила 0.63."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832cb36",
   "metadata": {},
   "source": [
    "## Обучение модели MultinomialNB на всех данных. Сохранение модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучим модель с подбором гиперпараметра alpha на 5 фолдах\n",
    "# на всей выборке и оценим качество\n",
    "X_full = pd.concat([X_train, X_test])\n",
    "y_full = pd.concat([y_train, y_test])\n",
    "param_grid = {'clf__alpha': np.arange(0.001, 0.3, 0.002)}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='f1_weighted')\n",
    "grid.fit(X_full, y_full)\n",
    "# выведем лучший score и значение гиперпараметра\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним модель в папку models\n",
    "dump(grid, './models/model_bayesNB.joblib')\n",
    "# сохраним классический oxford словарь\n",
    "dump(oxford.data, './oxford/classic_oxford.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724c976",
   "metadata": {},
   "source": [
    "## Выводы и рекомендации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ef897c",
   "metadata": {},
   "source": [
    "В ходе работы было выполнено:\n",
    "\n",
    "загрузка необходимых библиотек;\n",
    "загрузка и ознакомление с данными (загружены субтитры, фильмы с метками уровня сложности, классический словарь Oxford с 5000 словами);\n",
    "предобработка данных (очистка от дубликатов, проверка наличия разметки для обучающих данных, определение исходного количества представленных данных, очитска текста субтитров, разбивка на обучающую и тестовую выборки);\n",
    "препроцессинг данных, а именно преобразование текство субтитров с компактную разреженную матрицу с помощью CountVectorizer с гиперпараметром min_df=4, чтобы исключить специфические слова для определенных фильмов, которые не несут явной языковой нагрузки.;\n",
    "добавление дополнительных признаков- количество уникальных лемм (слов основной формы), содержащихся в каждом тексте субтитров согласно уровню сложности по классичесокму словарю Oxford;\n",
    "обоснование использования метрики качества F1-weighted;\n",
    "обучение модели-классификатора MultinomialNB с побором гиперпараметров на 5-и фолдах перекрестной прокерки;\n",
    "оценка модели на тестовой выборке;\n",
    "обучение модели на всей выборке с подбором гиперпарамтера на 5-и фолдах перекрестной прокерки\n",
    "сохранение лучшей модели в формате 'joblib', а так же классического словаря Oxford для дальнейшего развертывания в демонстрационном приложении Streamlit.\n",
    "\n",
    "При решении задачи обнаружены следующие проблеммы:\n",
    "\n",
    "Отсутствуют фильмы с уровнем сложности A1;\n",
    "В данных присутствует сильный дисбаланс по остальным классам (меткам):\n",
    "129 фильмов с уровнем сложности 3 (метка B2);\n",
    "56 фильмов с уровнем сложности 2 (метка B1);\n",
    "39 фильмов с уровнем сложности 4 (метка C1);\n",
    "32 фильма с уровнем сложности 1 (метка A2).\n",
    "Наличие большой вероятности в субъективности разметки данных в фильмах. Каждый специалист может немного по-разному определять уровень сложности понимания английского. В таком случае можно ожидать смещения в предсказании любой модели, так как субъективность часто вносит неустранимую ошибку в предиктивность модели. С большой долей вероятности модель будет допускать ошибки в предсказании. Особенно на граничных значениях меток (например ошибаться в присвоении метки A2 или B1, B1 или B2). Такие ошибки впринципе допустимы в отношении поставленной задачи, так как позволяют добиться того, чтобы студент понимал 50-70 % диалогов даже при ошибочном предсказании модели в сторону присвоения более низкой метки, так как различия между такими классами менее существенны.\n",
    "\n",
    "Выводы по результатам работы модели:\n",
    "\n",
    "Лучшая модель показала результат - 0.63 (F1_weighted) на тестовой выборке при подобранном гиперпараметре 'alpha' равном 0.137.\n",
    "\n",
    "как и ожидалось, модель чаще всего ошибается на граничных метках. Например модель часто назначает фильмам с меткой B2 метку B1. По условиям задачи где, студенты должны минимум понимать 50-70% диалогов, ошибка прогнозирования в сторону наименьшего класса допустима в пределах граничных классов;\n",
    "хуже всего классифицированы фильмы с метками A1 и B1. В первом случае за счет низкого значения метрики recall (среди небольшого количества фильмов представленных в этом классе меток (7 фильмов), всего 2 фильма размечены верно). Во втором случае за счет низкого значения метрики precision (модель часто ошибается в присвоении метки B1, т.е. среди всех размеченных фильмов с присвоением метки B1 (21 фильм) только 8 размечены верно.\n",
    "Использование дополнительно созданных признаков, с количеством уникальных слов из классического словаря Oxford в каждом фильме, не дали значительного прироста в качестве предсказаний, что может подтверждать предположение о субъективности разметки представленных Заказчиком фильмов, в силу предвзятости специалистов по английскому языку в отношении сложности диалогов.\n",
    "\n",
    "В целом с учетом предположения о субъективности разметки представленных Заказчиком фильмов, а так же использования быстрого и простого наивного Байесовского классификатора, удалось достичь приемлемых результатов. Взвешенная метрика F1-weighted на модели, обученной на всех представленных данных, при кроссвалидации на 5-и фолдах показала результат в 0.68 по метрике F1-weighted.\n",
    "\n",
    "Рекомендации:\n",
    "\n",
    "Собрать больше фильмов с размеченным уровнем сложности с одинаковым количеством в каждом классе, включая уровень A1. В данном случае можно добиться лучшей пргоностической способности модели;\n",
    "Разметка фильмов по уровню сложности должна быть лишена субъективности и опираться на общепринятые правила определения. Размеченные данные (фильмы) должны поступать от классифицированных специалистов по английскому языку, либо с общеизвесиных и общепринятых источников;\n",
    "(Предпочтительная и важная рекомендация для Заказчика) Для создания модели предсказания уровня сложности понимания фильмов необходимо полностью исключить заранее размеченные данные.\n",
    "В таком случае обучение модели будет основываться на других правилах, таких как:\n",
    "\n",
    "классический и американский словари Oxford,\n",
    "количество слов в предложениях,\n",
    "скорость речи,\n",
    "центральные оценки (среднее, медиана) количетства букв в слове в общем по всему тексту\n",
    "и т.д.\n",
    "Для такой разарботки модели потребуется дополнительное время (около 10 дней). В данном случае снижен риск субъективности в оценке сложности понимания английских фильмов, так как будут использованы более понятные и строгие правила определения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd950a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265c5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
